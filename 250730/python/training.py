# # *****************************ì„¤ëª…********************************* #
# # a ~ eëŠ” ëª¨ë¸ì„ ë§Œë“¤ê³  í•™ìŠµì‹œí‚¤ê³  ëª¨ë¸ì„ ì €ìž¥í•˜ëŠ” ê³¼ì •
# # a-1ì€ EMNIST ì „ë¶€, a-2ëŠ” (a,b,c ë§Œ) í›ˆë ¨í•˜ëŠ” ì½”ë“œ. ëª¨ë¸ í•™ìŠµ ì‹œí‚¬ ë•Œ ë‘˜ ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì£¼ì„!
# # d.ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ì„ ì„¤ì •í•´ ì¤„ ìˆ˜ ìžˆìŒ
# # f ë¶€í„°ëŠ” ì €ìž¥ëœ ëª¨ë¸ì˜ weight, biasë§Œ í™•ì¸í•˜ëŠ” ê³¼ì • 
# # ë§Œì•½ í•™ìŠµì„ ë‹¤ ì‹œí‚¨ì ì´ ìžˆê³  weight, biasë§Œ í™•ì¸í•˜ë ¤ë©´ a~e ì£¼ì„ì²˜ë¦¬í•˜ê³  í•˜ê¸°
# # ***************************************************************** #

import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
# datasets : MNIST ê°™ì€ ë°ì´í„° ì…‹ì„ ì‰½ê²Œ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
# transforms : ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œë„êµ¬



from torch.utils.data import DataLoader
# DataLoader : ë°ì´í„°ë¥¼ batch ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬(ë©”ëª¨ë¦¬ ì ˆì•½ + ë¹ ë¦„)

from PIL import ImageOps
from torchvision.datasets import EMNIST

#transform ì˜µì…˜
transform = transforms.Compose([
    transforms.Lambda(lambda img: ImageOps.invert(img)),  # ìƒ‰ ë°˜ì „
    transforms.Lambda(lambda img: ImageOps.mirror(img)),  # ì¢Œìš° ëŒ€ì¹­ ë³µì›
    transforms.Lambda(lambda img: img.rotate(90, expand=True)),  # ì‹œê³„ë°©í–¥ 90ë„ íšŒì „
    transforms.ToTensor()
#transform ì˜µì…˜
# MNISTëŠ” ê¸°ë³¸ì ìœ¼ PIL ì´ë¯¸ì§€(0~255) (Python Image Library)
# ì´ë¯¸ì§€ ì—´ê¸° (.jpg, .png, .bmp, .gif, .tiff, ...)
# í¬ê¸° ì¡°ì ˆ (resize)
# ìž˜ë¼ë‚´ê¸° (crop)
# íšŒì „, í‘ë°± ë³€í™˜, í•„í„° ì ìš©
# í”½ì…€ ê°’ ì ‘ê·¼
# TOTensor()ë¥¼ ì“°ë©´ ->[0.0, 1.0]ë²”ìœ„ì˜ í…ì„œë¡œ ë³€í™˜
])




# # ************** a-1. EMNIST ì „ì²´ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ************** #
# # *************************************************************** #
#
# # EMNIST ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆ: 'letters')
# train_dataset = EMNIST(root='./data', split='letters', train=True, download=True, transform=transform)
# test_dataset = EMNIST(root='./data', split='letters', train=False, download=True, transform=transform)
#
# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
#
# # *************************************************************** #
# # *************************************************************** #
# #
# #





# **************** a-2. A,B,Cë§Œ  í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ *************** #
# *************************************************************** #
target_labels = [1, 2, 3]
def filter_dataset(dataset):
    indices = [i for i, (_, label) in enumerate(dataset) if label in target_labels]
    filtered = Subset(dataset, indices)
    return filtered

# ì›ë³¸ ì „ì²´ EMNIST ë°ì´í„°ì…‹
full_train_dataset = EMNIST(root='./data', split='letters', train=True, download=True, transform=transform)
full_test_dataset = EMNIST(root='./data', split='letters', train=False, download=True, transform=transform)

# 'a', 'b', 'c'ë§Œ í•„í„°ë§
train_dataset = filter_dataset(full_train_dataset)
test_dataset = filter_dataset(full_test_dataset)

# Subsetì€ ë‚´ë¶€ì— indexë§Œ ë“¤ê³  ìžˆì–´ì„œ custom Dataset ê°ì‹¸ì¤˜ì•¼ í•¨
class ABCDataset(torch.utils.data.Dataset):
    def __init__(self, subset):
        self.subset = subset

    def __len__(self):
        return len(self.subset)

    def __getitem__(self, idx):
        img, label = self.subset[idx]
        return img, torch.tensor(label, dtype=torch.long)

train_dataset = ABCDataset(train_dataset)
test_dataset = ABCDataset(test_dataset)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
# *************************************************************** #
# *************************************************************** #








# ********************* b. CNN ëª¨ë¸ ë§Œë“¤ê¸° ************************ #
# *************************************************************** #

import torch.nn as nn
import torch.nn.functional as F


class CNN(nn.Module):
    def __init__(self, ch1, ch2):
        super(CNN, self).__init__()

        # pixel ë°ì´í„° ì¶”ê°€
        self.conv1 = nn.Conv2d(1, ch1, kernel_size=5, padding=0, bias=True)
        # í•©ì„±ê³± ë ˆì´ì–´

        self.pool = nn.MaxPool2d(2, 2)  # 2x2 Max pooling
        # MaxPool2d(2, 2): 2x2 ìµœëŒ€ í’€ë§ â†’ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ìž„

        self.conv2 = nn.Conv2d(ch1, ch2, kernel_size=5, padding=0, bias=True)

        self.fc1 = nn.Linear(ch2 * 4 * 4, 3, bias=True)  # ì™„ì „ì—°ê²°,  # a,b,c ë¶„ë¥˜
        # self.fc2 = nn.Linear(32, 3)  # a,b,c ë¶„ë¥˜

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        # x = x.view(-1, 3 * 4 * 4)  # Flatten
        x = x.view(x.size(0), -1)
        ##soft maxí•œê±°
        x = self.fc1(x)

        return x
# forward: ìž…ë ¥ì´ ëª¨ë¸ì„ í†µê³¼í•  ë•Œì˜ ì—°ì‚° ì •ì˜
# ReLU: ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ â†’ ë”¥ëŸ¬ë‹ì—ì„œ ë§¤ìš° ì¤‘ìš”
# view: í…ì„œë¥¼ íŽ¼ì³ì„œ FC ë ˆì´ì–´ì— ë„£ìŒ
# *************************************************************** #
# *************************************************************** #





# ******************* c.ì†ì‹¤í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜ ****************** #
# ************************************************************** #

import torch.optim as optim
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# device : ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ CPU, GPU ì–´ë””ì— ì˜¬ë¦´ì§€ ì •í•¨

# ************************************************************** #
# ************************************************************** #







# ********************** d.ëª¨ë¸ í›ˆë ¨ ë£¨í”„ ************************ #
# channel_configs => ì²« ë²ˆì§¸ kernerl ì±„ë„ ìˆ˜, ë‘ ë²ˆì§¸ kernel ì±„ë„ ìˆ˜
# epoch_list => epoch ì´ ëª‡ë²ˆ í• ê±´ì§€
# ex) epoch_list = [5,6,7] => channel_configs ê°ê°ì— ëŒ€í•˜ì—¬ epoch 5,6,7ë²ˆ í•™ìŠµì‹œí‚¤ê² ë‹¤
# ************************************************************** #
# í›ˆë ¨ ë£¨í”„
channel_configs = [
    (3, 3),
    # (8, 4),
    # (4, 8),
    # (8, 8),
    # (8, 16),
    # (16, 16),
]
epochs_list = [5]

for ch1, ch2 in channel_configs:
    for ep in epochs_list:
        print(f"\nâ–¶ï¸ Config: conv1={ch1}, conv2={ch2}, epochs={ep}")
        model = CNN(ch1, ch2).to(device)

        criterion = nn.CrossEntropyLoss()
        # ì˜ˆì¸¡ ì •ê°‘ ê°„ì˜ ì°¨ì´ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        # ì˜µí‹°ë§ˆì´ì €ëŠ” ëª¨ë¸ì˜ weightë¥¼ ì—…ë°ì´íŠ¸
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        for epoch in range(ep):
            model.train()
            total_loss = 0
            for images, labels in train_loader:
                images, labels = images.to(device), (labels - 1).to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"  Epoch [{epoch+1}/{ep}] Loss: {total_loss:.4f}")
# *************************ì •í™•ë„ í‰ê°€**************************** #
# ************************************************************** #
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), (labels - 1).to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
        acc = 100 * correct / total
        print(f"âœ… Accuracy: {acc:.2f}%")






# ********************** e.ëª¨ë¸ ì •í™•ë„ í‰ê°€ ë° ì €ìž¥ *********************** #
# ********************************************************************* #
        model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), (labels - 1).to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
        acc = 100 * correct / total
        print(f"âœ… Accuracy: {acc:.2f}%")


        # ðŸ”½ ëª¨ë¸ ì €ìž¥
        filename = f"cnn_c{ch1}_{ch2}_ep{ep}.pth"
        torch.save(model.state_dict(), filename)
        print(f"ðŸ“¦ Model saved as: {filename}")
# ************************************************************** #
# ************************************************************** #





# **************** f. í›ˆë ¨ ëª¨ë¸ weight, bias í™•ì¸ **************** #
# ************************************************************** #

# ë‚˜ì¤‘ì— í›ˆë ¨ëª¨ë¸ weightë§Œ í™•ì¸í•˜ê³  ì‹¶ì„ ë•Œ
model = CNN(ch1, ch2)

#ì›í•˜ëŠ” ëª¨ë¸ ì¸ë±ì‹±ìœ¼ë¡œ ì°¾ê¸°
# model.load_state_dict(torch.load("cnn_cX_X_epX.pth"))
# xë¡œ ë˜ì–´ìžˆëŠ”ê³³ì— ì›í•˜ëŠ” ìˆ«ìž ë„£ê¸° ( ex) ch1 = 3, ch2 = 3, ep =5 ì´ë©´ ì•„ëž˜ì²˜ëŸ¼)
model.load_state_dict(torch.load("cnn_c3_3_ep5.pth"))
model.eval()

# ëª¨ë¸ conv1ì˜ weight
print("=== Conv1 Weights ===")
print(model.conv1.weight.shape)  # => torch.Size([16, 1, 3, 3])
print(model.conv1.weight)  # => ì‹¤ì œ ê°’ ì¶œë ¥

print("=== Conv2 Weights ===")
print(model.conv2.weight.shape)  # => torch.Size([16, 1, 3, 3])
print(model.conv2.weight)  # => ì‹¤ì œ ê°’ ì¶œë ¥

print("\n=== FC1 Weights ===")
print(model.fc1.weight.shape)  # ex) torch.Size([128, 1568])
print(model.fc1.weight)

print("\n=== bias ===")
print("conv1 bias:", model.conv1.bias.data)
print("conv2 bias:", model.conv2.bias.data)
print("fc1 bias:", model.fc1.bias.data)
# ************************************************************** #
# ************************************************************** #


