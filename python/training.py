# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
import os

import numpy as np
import torch

from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
# datasets : MNIST ê°™ì€ ë°ì´í„° ì…‹ì„ ì‰½ê²Œ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
# transforms : ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œë„êµ¬

from PIL import Image

from torch.utils.data import DataLoader
# DataLoader : ë°ì´í„°ë¥¼ batch ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬(ë©”ëª¨ë¦¬ ì ˆì•½ + ë¹ ë¦„)
from PIL import ImageOps

import matplotlib.pyplot as plt

# ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
from torchvision.datasets import EMNIST

transform = transforms.Compose([
    transforms.Lambda(lambda img: ImageOps.invert(img)),  # ìƒ‰ ë°˜ì „
    transforms.Lambda(lambda img: ImageOps.mirror(img)),  # ì¢Œìš° ëŒ€ì¹­ ë³µì›
    transforms.Lambda(lambda img: img.rotate(90, expand=True)),  # ì‹œê³„ë°©í–¥ 90ë„ íšŒì „
    transforms.ToTensor()
])
# MNISTëŠ” ê¸°ë³¸ì ìœ¼ PIL ì´ë¯¸ì§€(0~255) (Python Image Library)
# ì´ë¯¸ì§€ ì—´ê¸° (.jpg, .png, .bmp, .gif, .tiff, ...)
# í¬ê¸° ì¡°ì ˆ (resize)
# ì˜ë¼ë‚´ê¸° (crop)
# íšŒì „, í‘ë°± ë³€í™˜, í•„í„° ì ìš©
# í”½ì…€ ê°’ ì ‘ê·¼
# TOTensor()ë¥¼ ì“°ë©´ ->[0.0, 1.0]ë²”ìœ„ì˜ í…ì„œë¡œ ë³€í™˜


# ==============================í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹=======================================#
# EMNIST ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆ: 'letters')
# train_dataset = EMNIST(root='./data', split='letters', train=True, download=True, transform=transform)
# test_dataset = EMNIST(root='./data', split='letters', train=False, download=True, transform=transform)
#
# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

target_labels = [1, 2, 3]


def filter_dataset(dataset):
    indices = [i for i, (_, label) in enumerate(dataset) if label in target_labels]
    filtered = Subset(dataset, indices)
    return filtered


# ì›ë³¸ ì „ì²´ EMNIST ë°ì´í„°ì…‹
full_train_dataset = EMNIST(root='./data', split='letters', train=True, download=True, transform=transform)
full_test_dataset = EMNIST(root='./data', split='letters', train=False, download=True, transform=transform)

# 'a', 'b', 'c'ë§Œ í•„í„°ë§
train_dataset = filter_dataset(full_train_dataset)
test_dataset = filter_dataset(full_test_dataset)


# Subsetì€ ë‚´ë¶€ì— indexë§Œ ë“¤ê³  ìˆì–´ì„œ custom Dataset ê°ì‹¸ì¤˜ì•¼ í•¨
class ABCDataset(torch.utils.data.Dataset):
    def __init__(self, subset):
        self.subset = subset

    def __len__(self):
        return len(self.subset)

    def __getitem__(self, idx):
        img, label = self.subset[idx]
        return img, torch.tensor(label - 1, dtype=torch.long)


# âœ… ë¬¸ì œ í•µì‹¬ ìš”ì•½
# PyTorchì˜ CrossEntropyLossëŠ” **ë¼ë²¨(label)**ì´ ë°˜ë“œì‹œ
# torch.LongTensor íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
#
# í•˜ì§€ë§Œ í˜„ì¬ ABCDatasetì˜ __getitem__()ì€ label - 1ë§Œ ìˆ˜í–‰í•˜ê³ ,
# ìë£Œí˜•(int, float)ì€ torch.Tensorì¸ì§€ í™•ì‹¤ì¹˜ ì•ŠìŠµë‹ˆë‹¤.


train_dataset = ABCDataset(train_dataset)
test_dataset = ABCDataset(test_dataset)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
# ======================================================================================#

# 1. CNN ëª¨ë¸ ë§Œë“¤ê¸°

import torch.nn as nn
import torch.nn.functional as F


class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()

        # pixel ë°ì´í„° ì¶”ê°€
        self.conv1 = nn.Conv2d(1, 3, kernel_size=5, padding=0, bias=True)
        # í•©ì„±ê³± ë ˆì´ì–´

        self.pool = nn.MaxPool2d(2, 2)  # 2x2 Max pooling
        # MaxPool2d(2, 2): 2x2 ìµœëŒ€ í’€ë§ â†’ í¬ê¸°ë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„

        self.conv2 = nn.Conv2d(3, 3, kernel_size=5, padding=0, bias=True)

        self.fc1 = nn.Linear(3 * 4 * 4, 3, bias=True)  # ì™„ì „ì—°ê²°,  # a,b,c ë¶„ë¥˜
        # self.fc2 = nn.Linear(32, 3)  # a,b,c ë¶„ë¥˜

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        # x = x.view(-1, 3 * 4 * 4)  # Flatten
        x = x.view(x.size(0), -1)
        ##soft maxí•œê±°
        x = self.fc1(x)

        return x


# forward: ì…ë ¥ì´ ëª¨ë¸ì„ í†µê³¼í•  ë•Œì˜ ì—°ì‚° ì •ì˜
# ReLU: ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ â†’ ë”¥ëŸ¬ë‹ì—ì„œ ë§¤ìš° ì¤‘ìš”
# view: í…ì„œë¥¼ í¼ì³ì„œ FC ë ˆì´ì–´ì— ë„£ìŒ


################################################## ì£¼ì„ì²˜ë¦¬ êµ¬ë¶„ ##################################################
######### ì†ì‹¤í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜ ##########
# model = CNN()
#
# # device : ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ CPU, GPU ì–´ë””ì— ì˜¬ë¦´ì§€ ì •í•¨
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#
# # modelì„ ì˜®ê¸°ëŠ” ì½”ë“œ
# model.to(device)
# import torch.optim as optim
# # ì˜ˆì¸¡ ì •ê°‘ ê°„ì˜ ì°¨ì´ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
# criterion = nn.CrossEntropyLoss()
# # ì˜µí‹°ë§ˆì´ì €ëŠ” ëª¨ë¸ì˜ weightë¥¼ ì—…ë°ì´íŠ¸
# optimizer = optim.Adam(model.parameters(), lr=0.001)
#
#
# # í›ˆë ¨ ë£¨í”„
# epochs = 5
#
# for epoch in range(epochs):
#     model.train()
#     total_loss = 0
#
#     for images, labels in train_loader:
#         images, labels = images.to(device), (labels).to(device)
#
#         outputs = model(images)
#         loss = criterion(outputs, labels)
#
#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
#
#         total_loss += loss.item()
#
#     print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}")
# ########################ëª¨ë¸ ì •í™•ë„ í‰ê°€##############################
# model.eval()
# correct = 0
# total = 0
#
# with torch.no_grad():
#     for images, labels in test_loader:
#         images, labels = images.to(device), (labels).to(device)
#         outputs = model(images)
#         _, predicted = torch.max(outputs.data, 1)
#         total += labels.size(0)
#         correct += (predicted == labels).sum().item()
# print(f"Test Accuracy: {100 * correct / total:.2f}%")
#
#
# # 1ë‹¨ê³„ í›ˆë ¨í•œ ëª¨ë¸ ì €ì¥í•˜ëŠ” ì½”ë“œ
# # "mnist_cnn.pth" íŒŒì¼ì´ ìƒì„±ë¨ (ì´ê²Œ ëª¨ë¸ì˜ weight ì €ì¥ë³¸ì´ì—ìš”
# torch.save(model.state_dict(), "mnist_cnn.pth")
################################################## ì£¼ì„ì²˜ë¦¬ êµ¬ë¶„ ##################################################


#######ì €ì¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°#########
# model = CNN()
# model.load_state_dict(torch.load("mnist_cnn.pth"))
# model.eval()
#################################


#############################################################
model = CNN()

# Conv1 weights and bias
model.conv1.weight.data = torch.tensor([
    [[[12, 13, 33, 20, 42], [11, 24, 54, 33, 28], [45, 51, 19, 8, 22], [-14, 14, -22, -20, -8],
      [-37, -21, -22, -42, -50]]],
    [[[23, 21, -11, 25, 26], [27, 4, 16, -1, 26], [2, 20, -3, 9, 61], [44, 8, 35, 31, 15], [34, 47, 43, 31, 46]]],
    [[[1, 0, -4, 39, 23], [0, 9, 33, 47, 51], [4, -15, 20, 41, -1], [-49, -37, 17, -16, -7], [-52, -16, -48, -32, -19]]]
], dtype=torch.float32)
model.conv1.bias.data = torch.tensor([25, -41, 43], dtype=torch.float32)

# Conv2 weights and bias
model.conv2.weight.data = torch.tensor([[[[0.2592, 0.0712, 0.0715, -0.0580, 0.0023],
                                          [0.2311, 0.1724, -0.1054, 0.2066, 0.2696],
                                          [0.1158, 0.0411, 0.2065, 0.3031, 0.3592],
                                          [0.1499, 0.2090, 0.2459, 0.2897, 0.2585],
                                          [-0.0133, 0.0900, 0.0635, -0.0385, -0.0221]],
                                         [[0.1137, 0.1231, 0.1111, 0.1404, 0.2687],
                                          [0.1210, -0.0417, 0.1161, 0.2051, 0.1154],
                                          [-0.2439, 0.1009, -0.0055, 0.0110, 0.1335],
                                          [-0.1489, -0.0715, -0.1878, -0.1183, -0.0183],
                                          [-0.2312, -0.2451, -0.0377, -0.1718, -0.1398]],
                                         [[0.0295, -0.1335, 0.0305, -0.0602, -0.0554],
                                          [0.2743, -0.2187, -0.2796, -0.1247, 0.1965],
                                          [0.2387, 0.2164, 0.1097, 0.3045, 0.3437],
                                          [0.3677, 0.3251, 0.3557, 0.2930, 0.2443],
                                          [0.3595, 0.4071, 0.2222, -0.0350, -0.0430]]],
                                        [[[-0.1237, -0.2287, -0.2918, -0.0458, -0.1907],
                                          [-0.2162, -0.2087, -0.0187, 0.0290, 0.0375],
                                          [-0.0691, -0.0302, -0.0080, -0.0522, 0.1312],
                                          [0.0865, 0.0529, -0.1573, -0.0454, 0.0332],
                                          [0.1930, 0.0527, 0.1166, -0.0421, 0.1244]],
                                         [[-0.0041, -0.2357, -0.0787, 0.1704, 0.0813],
                                          [0.0986, 0.0840, -0.0510, 0.0084, 0.2955],
                                          [0.2652, 0.0935, 0.0479, 0.0579, 0.2364],
                                          [0.2606, 0.2075, 0.2360, 0.0723, 0.1834],
                                          [0.4022, 0.1996, 0.2317, 0.1236, 0.0708]],
                                         [[-0.2238, 0.0464, -0.0619, -0.1580, -0.4147],
                                          [-0.2716, -0.1588, 0.0247, -0.0309, -0.3717],
                                          [-0.3569, -0.3113, -0.0601, 0.0496, 0.0399],
                                          [-0.5184, -0.4376, -0.4133, -0.2760, -0.2177],
                                          [-0.3189, -0.3563, -0.3204, -0.3664, -0.0879]]],
                                        [[[-0.0726, 0.0422, 0.0626, 0.1008, -0.0765],
                                          [-0.0648, -0.1228, -0.0185, 0.0808, 0.0234],
                                          [-0.0309, 0.0880, 0.0239, -0.0979, 0.0564],
                                          [0.0055, -0.1021, 0.0080, 0.0100, -0.0107],
                                          [-0.0768, -0.1091, -0.1176, 0.0032, -0.0478]],
                                         [[0.0858, 0.0830, -0.0584, -0.0989, 0.0354],
                                          [-0.1199, 0.0478, -0.1170, -0.0730, 0.0468],
                                          [-0.0674, -0.0596, -0.0013, -0.0062, -0.0791],
                                          [-0.0726, 0.0252, 0.0043, 0.0175, -0.0742],
                                          [-0.0150, -0.0348, -0.0256, 0.0174, -0.1215]],
                                         [[0.0413, 0.0130, 0.0275, -0.1187, -0.0725],
                                          [-0.1053, 0.0121, 0.0139, -0.0512, -0.0700],
                                          [0.0062, 0.0559, -0.0301, -0.0066, -0.1207],
                                          [0.0778, -0.0234, 0.0898, -0.1014, 0.0780],
                                          [-0.0589, 0.0113, 0.0741, -0.0934, -0.0371]]]], dtype=torch.float32)
model.conv2.bias.data = torch.tensor([0.0135, -0.1800, 0.0046], dtype=torch.float32)

# FC1
model.fc1.weight.data = torch.tensor([
    [0.0358, -0.0892, -0.1154, 0.2058, 0.2404, 0.0259, -0.1440, 0.0059,
     0.2153, -0.1251, -0.1559, 0.0405, 0.1184, -0.1408, -0.2498, 0.0234,
     0.0066, -0.0768, 0.0770, 0.0476, 0.0091, 0.1527, -0.1512, -0.0963,
     -0.1414, 0.0247, -0.0792, -0.2076, 0.2913, 0.0852, 0.2078, -0.0901,
     0.1050, -0.0120, -0.0286, 0.0388, -0.1076, 0.1021, -0.1197, -0.0495,
     0.0179, 0.0120, -0.1090, -0.1356, 0.0408, -0.0453, 0.0429, 0.0946],

    [-0.1567, 0.1298, 0.2034, 0.0122, -0.0825, -0.0784, 0.1906, 0.1770,
     -0.0039, -0.1989, 0.0416, -0.1426, 0.0703, 0.0728, 0.1167, -0.1333,
     0.0852, 0.0266, 0.1161, -0.0150, 0.1577, -0.1318, -0.1142, -0.0008,
     0.2083, 0.0231, -0.1867, 0.0559, -0.1664, -0.0917, -0.1456, -0.1681,
     0.0595, 0.1351, -0.0901, 0.0638, -0.1194, 0.0442, 0.0220, 0.0419,
     0.0706, 0.0397, -0.0872, 0.1021, -0.0406, -0.1114, 0.0385, -0.1103],

    [0.1582, -0.0269, -0.1086, -0.0754, -0.1537, -0.0654, -0.0743, -0.1402,
     -0.0544, 0.1673, 0.1356, 0.0997, -0.1909, -0.1407, -0.0803, 0.0386,
     0.0003, -0.0249, 0.0890, -0.0099, -0.2609, 0.1044, 0.2266, 0.1749,
     -0.0273, 0.0117, 0.1234, 0.1758, -0.0406, -0.1432, 0.0826, -0.1058,
     0.0456, -0.0198, -0.0950, 0.0574, -0.0639, -0.0173, 0.0196, -0.1012,
     0.1156, 0.0139, -0.0454, -0.1070, -0.0792, -0.1303, -0.0382, 0.1213]
], dtype=torch.float32)

model.fc1.bias.data = torch.tensor([11, -7, -15], dtype=torch.float32)

# í…ŒìŠ¤íŠ¸ ì¶œë ¥
print("Conv1 Filter 0:\n", model.conv1.weight.data[0])
print("FC1 Weights for Class 0:\n", model.fc1.weight.data[0])

#############################################################


# 2ë‹¨ê³„: ë‚˜ì¤‘ì— weightë§Œ í™•ì¸í•˜ê³  ì‹¶ì„ ë•Œ
# model = CNN()
# model.load_state_dict(torch.load("mnist_cnn.pth"))
model.eval()
print(model.conv1.weight.shape)  # torch.Size([16, 1, 3, 3])
print(model.conv1.weight[0])  # ì²« ë²ˆì§¸ í•„í„° ë³´ê¸°

# ëª¨ë¸ conv1ì˜ weight
print("=== Conv1 Weights ===")
print(model.conv1.weight.shape)  # => torch.Size([16, 1, 3, 3])
print(model.conv1.weight)  # => ì‹¤ì œ ê°’ ì¶œë ¥

print("=== Conv2 Weights ===")
print(model.conv2.weight.shape)  # => torch.Size([16, 1, 3, 3])
print(model.conv2.weight)  # => ì‹¤ì œ ê°’ ì¶œë ¥

print("\n=== FC1 Weights ===")
print(model.fc1.weight.shape)  # ex) torch.Size([128, 1568])
print(model.fc1.weight)

print("\n=== bias ===")
print("conv1 bias:", model.conv1.bias.data)
print("conv2 bias:", model.conv2.bias.data)
print("fc1 bias:", model.fc1.bias.data)

# print("\n=== FC2 Weights ===")
# print(model.fc2.weight.shape)     # ex) torch.Size([10, 128])
# print(model.fc2.weight)

#################### ë‚´êº¼ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸##################
transform = transforms.Compose([
    # transforms.Grayscale(num_output_channels=1),
    transforms.Resize((28, 28)),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
# ì •ê·œí™” ì—†ëŠ” ë³€í™˜ (ToTensorê¹Œì§€ë§Œ)
raw_transform = transforms.Compose([
    transforms.Resize((28, 28)),
    transforms.ToTensor()
])

def binarize(img, threshold=128):
    return img.point(lambda p: 255 if p > threshold else 0)


image_folder = "C:/Users/kccistc/Desktop/handwritebold/"
# ì‚¬ëŒ ìˆ˜
num_people = 4

# íŒŒì¼ëª… ìƒì„±: a_1.png ~ c_3.png
file_names = [f"{chr(ch)}_{i}.png"
              for i in range(1, num_people + 1)
              for ch in range(ord('a'), ord('c') + 1)]

for file in file_names:
    img_path = image_folder + file
    img = Image.open(img_path)

    img = Image.open(img_path).convert("L")
    # ì´ë¯¸ì§€ë¥¼ 8bit í‘ë°±ìœ¼ë¡œ ë³€í™˜

    # img = ImageOps.invert(img)
    # # ì´ë¯¸ì§€ë¥¼ ìƒ‰ìƒ ë°˜ì „(í‘ë°±ì´ì—¬ì•¼ ê°€ëŠ¥)

    # img = ImageOps.pad(img, (28, 28), centering=(0.5, 0.5), color = 0)
    # ì…ë ¥ ì´ë¯¸ì§€ê°€ ì´ê²ƒë³´ë‹¤ ì‘ìœ¼ë©´ íŒ¨ë”© ë° ê°€ìš´ë° ì •ë ¬
    # í¬ë”ë¼ë„ ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ ì¶•ì†Œí•˜ëŠ”ë° ì´ ë•Œ ë¹ˆê³µê°„ íšŒìƒ‰ìœ¼ë¡œ ì±„ì›€

    # img = binarize(img, threshold=128)
    # # ì´ë¯¸ì§€ ì´ì§„í™”

    img_tensor = transform(img).unsqueeze(0)
    img_np = img_tensor.squeeze().numpy()  # [1, 28, 28] â†’ [28, 28]
    # ğŸ”¸ 8bit mem íŒŒì¼ë¡œ ì €ì¥

    # (1) ì •ê·œí™” ì „ ì´ë¯¸ì§€ â†’ .mem ì €ì¥ìš©
    raw_tensor = raw_transform(img)
    raw_np = raw_tensor.squeeze().numpy()  # [28, 28]
    raw_np_255 = (raw_np * 255).astype(np.uint8)
    # .mem ì €ì¥
    mem_filename = file.replace(".png", ".mem")
    save_path = f"./mem_outputs/{mem_filename}"
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    with open(save_path, "w") as f:
        for y in range(28):
            for x in range(28):
                val = raw_np_255[y, x]
                f.write(f"0x{val:02X}\n")
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")
    plt.close('all')
    # imshow()ëŠ” í‘ë°±ì´ì–´ë„ ì»¬ëŸ¬ë¡œ ë³´ì—¬ì¤„ë ¤ê³  í•¨
    plt.imshow(img_np, cmap="gray")
    plt.title(f"Resized Image: {file}")
    plt.axis("off")
    plt.show()
    output = model(img_tensor)
    _, predicted = torch.max(output, 1)

    predicted_letter = chr(predicted.item() + ord('a'))

    print("Output logits:", output)
    print(f"{file} â†’ Predicted digit: {predicted_letter}")
    print()

# for i in range(3):
#     image, label = train_dataset[i]  # label: 1~26 (a~z)
#     char_label = chr(label + ord('a'))  # ASCII ë§¤í•‘
#
#     plt.close('all')
#     #imshow()ëŠ” í‘ë°±ì´ì–´ë„ ì»¬ëŸ¬ë¡œ ë³´ì—¬ì¤„ë ¤ê³  í•¨
#     plt.imshow(image.squeeze(), cmap= "gray")
#     plt.title(f"Label: {char_label} (index: {label})")
#     plt.axis("off")
#     plt.show()
#     plt.close('all')
######################################################
print("end")


